{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nHello everyone, I will be sharing my approach of using a LLM for this competition. Though it was not my best solution, but I still think with some amount of work this can become a really great one. So feel free to tweak it and see how it performs.\n\nI have not used any external data, and I had tried running this notebook on Kaggle but it didn't run, so I had used A5000 with 24GB of VRAM. To train the non-quantized version you would require roughly 60-70GB of VRAM to finetune the model. Suggestions you be 1xA100-80GB/2xA100-40GB/3xA100-40GB/8xA4000-16GB/8xV100-32GB. You can also opt in for the latest version including A6000-ADA, H100, A4000-ADA","metadata":{}},{"cell_type":"markdown","source":"## Required Libraries\n\nEnsure when you are running this notebook the following libraries are present with their latest versions.\n* Transformers \n* Datasets\n* BitsandBytes\n* Accelerate\n* Sentencepiece\n* PeFT","metadata":{}},{"cell_type":"code","source":"#Setting Up the Model String for the model to be used\n'''\n                       I have used the Base Mistral Model, but you can also choose the Intruct Model.\n                    One More suggestion would be any trial with the Zephyr-Alpha/Beta Model from HF-H4\n'''\n\nmodel_str = \"mistralai/Mistral-7B-v0.1\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing the required Libraries\n\n#Analysis and data creation\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\n\n#Modelling\nimport torch\nfrom transformers import AutoTokenizer\nfrom transformers import MistralForSequenceClassification #LlamaForSequenceClassification, AutoModelForSequenceClassification [Llama head works most of the time but the results are not that good, AutoModel invokes the Mistral native head]\nfrom transformers import TrainingArguments, Trainer, EarlyStoppingCallback\nfrom transformers import DataCollatorWithPadding\nfrom tqdm import tqdm\n\n#Quantization\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType \nfrom transformers import BitsAndBytesConfig\nimport torch\n\n#Model Storage\nfrom shutil import rmtree\nfrom pathlib import Path","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reading the Train and Test Data\n\ntrain = pd.read_csv(\"/kaggle/input/h2oai-predict-the-llm/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/h2oai-predict-the-llm/test.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Replacing the NaN values with NA String, you can use Empty String or any other sequence\ntrain.fillna(\"NA\",inplace=True)\ntest.fillna(\"NA\",inplace=True)\n\n#Merging the Two Columns together, Alternatively you can use them as separate input for the model as well.\ntrain[\"ques_resp\"] = 'Question: ' + train[\"Question\"] + \"; \" + 'Response: ' + train[\"Response\"]\ntest[\"ques_resp\"] = 'Question: ' + test[\"Question\"] + \"; \" + 'Response: ' + test[\"Response\"]\n\n#Creating another dataframe with only the required columns.\ntrain_merged = train[[\"target\",\"ques_resp\"]]\ntest_merged = test[[\"ques_resp\"]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the following cell I have dropped the rows in the Train Set that corresponded to *Label 4* as during an initial analysis I found that *Label 4* had near to zero if not zero appearances in Test Set. \nFor a more detailed analysis please check out this [Notebook](https://www.kaggle.com/code/mustafakeser4/basic-eda)","metadata":{}},{"cell_type":"code","source":"train_merged.drop(train_merged[train_merged['target'] == 4].index, inplace = True)\ntrain_merged","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating the HF Dataset corresponding to our train and test\n\ntrain_hf_dataset = Dataset.from_pandas(train_merged)\ntest_hf_dataset = Dataset.from_pandas(test_merged)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Setting up the PeFT and BnB config\n\n'''\nThe PeFT Config will allow us to create a LoRA Adapter for our quantized model that will allow us to train it\nThe BnB Config will load the model in 4-Bit quantization thus reducing our hardware requirement from 60GB to only 13GB\n'''\n\npeft_config = LoraConfig(\n    r=64,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    target_modules=[\n        \"q_proj\",\n        \"v_proj\"\n    ],\n)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_str, use_fast=False)\n\n#Addition of the Pad Token as the Tokenizer.pad_token is Set to None, this is required to ensure efficient padding\ntokenizer.pad_token = tokenizer.eos_token","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading the Model\nmodel_quantized = MistralForSequenceClassification.from_pretrained(model_str, num_labels=train.target.nunique(), quantization_config=bnb_config, device_map={\"\":0})\n\n#Setting the Pretraining_tp to 1 ensures we are using the Linear Layers to the max computation possible\nmodel_quantized.config.pretraining_tp = 1 #For Us this would be 7B\n\n#Ensuring the model is aware about the pad token ID\nmodel_quantized.config.pad_token_id = tokenizer.pad_token_id","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Setting up the LoRA Adapter\nmodel_main = get_peft_model(model_quantized, peft_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_main.print_trainable_parameters()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length =512 #I have used a length of 512 due to memory concerns you can also use higher ranges or you this->tokenizer.model_max_length to invoke the max length of the model\n\n#Tokenizing the Datasets\ndef tokenize_function(examples):\n    return tokenizer(examples[\"ques_resp\"], padding=\"max_length\", max_length = max_length, truncation=True)\n'''\nIf you are using two separate columns you can use the following method\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"question\"], examples[\"response\"] padding=\"max_length\", max_length = max_length, truncation=True)\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_train = train_hf_dataset.map(tokenize_function, batched=True)\ntokenized_test = test_hf_dataset.map(tokenize_function, batched=True)\n\ntokenized_train_main = tokenized_train.shuffle(seed=42).select(range(0,2726))\ntokenized_eval_main = tokenized_train.shuffle(seed=42).select(range(2726,3408))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_train_main = tokenized_train_main.remove_columns(['ques_resp'])\ntokenized_train_main = tokenized_train_main.rename_column(\"target\", \"labels\")\n\n\ntokenized_eval_main = tokenized_eval_main.remove_columns(['ques_resp'])\ntokenized_eval_main = tokenized_eval_main.rename_column(\"target\", \"labels\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I have used the following training arguments, you can tweak and see if there are any differences in the results\nsteps = 20\n\nrun_name = \"mistral\" + \"-\" + \"h2o-finetune\"\noutput_dir = \"./\" + run_name\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=5e-5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=16,\n    max_grad_norm=0.3,\n    optim='paged_adamw_32bit',\n    lr_scheduler_type=\"cosine\",\n    num_train_epochs=8,\n    weight_decay=0.01,\n    evaluation_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n    warmup_steps=steps,\n    eval_steps=steps,\n    logging_steps=steps,\n    report_to='none'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Shifting the model to GPU, you can do this while invoking the model as well\nmodel_main.to('cuda')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up the Trainer API\ntrainer = Trainer(\n    model=model_main,\n    args=training_args,\n    train_dataset=tokenized_train_main,\n    eval_dataset=tokenized_eval_main,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\n#It took roughly 3 hours and 49 mins for me to train on this particular dataset using A5000\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sometimes Quantized models can save steps, to ensure you save the best model, you can run this command once.\n\nfrom shutil import rmtree\nfrom pathlib import Path\ntrainer.save_model(output_dir=str(output_dir))\n\nfor path in Path(training_args.output_dir).glob(\"checkpoint-*\"):\n    if path.is_dir():\n        rmtree(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing the text column from the test dataset\ntokenized_test_main = tokenized_test.remove_columns(['ques_resp'])\n\n#Using the trainer to predict the output, it will used the default eval batch number\npred = trainer.predict(tokenized_test_main)\n\n#Getting the probabilities\npred_proba =torch.nn.functional.softmax(torch.tensor(pred.predictions, dtype = torch.float), dim=1)\n#You have to set dtype for torch.tensor only when using a quantized model as nn.softmax/logmax doesn't apply for half-precision\n\n#Saving the probabilities for submission\nsubmission = pd.read_csv(\"/kaggle/input/h2oai-predict-the-llm/sample_submission.csv\",index_col=\"id\")\nsubmission[:] = pred_proba.cpu().numpy()\nsubmission.to_csv(\"submission.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The End\n\nKudos you have completed training and submitting to [h2oai-predict-the-llm](https://www.kaggle.com/competitions/h2oai-predict-the-llm/overview) using a quantized model.","metadata":{}}]}